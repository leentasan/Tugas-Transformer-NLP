# üöÄ Implementasi Transformer dari Nol dengan NumPy

Proyek ini merupakan **implementasi arsitektur GPT-style Decoder-only Transformer** dari nol (*from scratch*) hanya menggunakan **NumPy**.  
Dibuat untuk memenuhi tugas individu mata kuliah **Natural Language Processing**.

Implementasi ini mencakup seluruh komponen fundamental dari arsitektur Transformer, mulai dari embedding hingga prediksi token berikutnya, serta telah divalidasi menggunakan suite pengujian yang komprehensif.

---

## üë©‚Äçüíª Penulis
**Della Febi Alfian**  
NIM: 22/505892/TK/55393

---

## Fitur Utama

- **Implementasi Murni NumPy**  
  Tidak menggunakan library deep learning seperti PyTorch atau TensorFlow.

- **Arsitektur Lengkap**  
  Mencakup 8 komponen utama Transformer:
  1. Token Embedding  
  2. Positional Encoding (Sinusoidal)  
  3. Scaled Dot-Product Attention  
  4. Multi-Head Attention  
  5. Feed-Forward Network (aktivasi **GELU**)  
  6. Residual Connections & Layer Normalization (Pre-Norm)  
  7. Causal Masking  
  8. Output Layer & Softmax  

- **Pengujian Komprehensif**  
  Dilengkapi dengan `test.py` untuk memvalidasi kebenaran matematis, fungsionalitas, dan stabilitas implementasi.

- **Fitur Bonus**  
  Termasuk visualisasi *causal self-attention pattern* untuk menunjukkan fungsionalitas masking.

---

## Struktur Folder
- transformer.py # Implementasi inti Transformer (semua komponen)
- test.py # Suite pengujian komprehensif
- requirements.txt # Daftar dependensi (numpy, matplotlib)
- attention_pattern.png # Contoh visualisasi pola atensi


---

## Cara Menjalankan Program

### 1 Prasyarat
Pastikan Anda sudah menginstal **Python 3.8** atau versi yang lebih baru.

---

### Ô∏è2 Instalasi Dependensi

Hanya ada dua dependensi utama:
- `numpy` untuk operasi matematis  
- `matplotlib` untuk visualisasi  

Buka terminal di folder proyek, lalu jalankan:
```bash
pip install -r requirements.txt
Atau instal secara manual:
pip install numpy matplotlib

Ô∏è### 3 Menjalankan Uji Coba Sederhana

Skrip utama transformer.py berisi uji coba sederhana untuk memvalidasi alur forward pass dan menghasilkan visualisasi pola atensi.

Jalankan perintah berikut:

python transformer.py

Setelah dijalankan:

Program akan mencetak hasil tes dari setiap komponen Transformer.

File attention_pattern.png akan muncul di folder proyek Anda, menampilkan pola self-attention.

---

### Ô∏è4 Menjalankan Suite Pengujian Komprehensif

Untuk menjalankan pengujian lengkap terhadap semua komponen, gunakan perintah:

```bash
python test.py

Jika semua tes berhasil, akan muncul pesan berikut di akhir output:
ALL TESTS PASSED!


## Hasil Visualisasi

Contoh hasil visualisasi causal self-attention pattern yang dihasilkan oleh model:
![Attention Pattern](attention_pattern.png)
